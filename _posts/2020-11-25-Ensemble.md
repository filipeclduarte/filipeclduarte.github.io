# Ensemble Learning

According to Kuncheva (2003), "combining classifiers is now an established research area known under different names in the literature: committees of learners, mixtures of experts, classifier ensembles, multiple classifier systems, consensus theory, etc."

Combine models achieve better results than using one model for classification Kuncheva (2014). Also, an ensemble classifier is just one classifier model.

**Reasons to combine models**

 I. **Statistical**: It is better to average several models than choose only one. 
 
 II. **Computational**: Small sample size or too much data; Imperfect training algorithm. 
 
 III. **Representational**: Complex boundary can be approximated with the desired precision by simple boundaries. An ensemble of linear classifiers can approximate a highly nonlinear classification boundary. 

**Ensemble Learning consists of three phases**

1. Generation

 It is preferable to use simpler models like Perceptrons or Decision Trees for the pool to guarantee diversity. We could use a "monolithic" model or not, for example, using several different algorithms. 
  
**Bagging** is an ensemble that combines multiple models based on bootstrap samples. In this sense, it is possible to use a monolith or several different classifiers algorithms. 

**Random subspace** is an ensemble that combines multiple models based on random feature selection. For each classifier in an 'L' pool of classifiers, sample 'n' features to training. 

2. Selection

In classifier **Selection**, each classifier is supposed to know some part of the feature space. 

3. Fusion

In classifier **Fusion**, each classifier is supposed to know the whole feature space. 



**Random Linear Oracle**

The Random Linear Oracle (RLO) is a method that combines the classifier fusion and classifier selection approaches (Kuncheva, 2014). Linear Oracle is a mini-ensemble consisting of only two classifiers, which choose the best classifier for a given training instance. 

![Kuncheva (2014)](/images/RLO.png)


### Measures of Diversity

Diversity is an essential characteristic in ensemble learning (KUNCHEVA, 2003, Cunningham & Carney, 2000; Krogh & Vedelsby, 1995; Rosen, 1996; Lam, 2000; Littlewood & Miller, 1989). However, there is no formal definition of what it represents. In this sense, Kuncheva (2003) tries to resolve these issues and answer some questions about this topic, such as i) how we define diversity? ii) How are the various measures of diversity-related to each other? iii) How are the measures related to the accuracy of the team? iv) Is there a measure that is best to develop committees that minimize error? v) How can we use the measures in designing the classifier ensemble?


#### Diversity in classifier ensemble 

Assume that classifier outputs are estimates of the posterior probabilities, ![P(\omega|\textbf{x})](https://latex.codecogs.com/gif.latex?P(\omega|\textbf{x})), s = 1, ..., c, so that the estimate satisfies:

![\hat{P}_i(\omega_s|\textbf{x})=P(\omega_s|\textbf{x})+\eta_s^i(\textbf{x})](https://latex.codecogs.com/gif.latex?\hat{P}_i(\omega_s|\textbf{x})=P(\omega_s|\textbf{x})+\eta_s^i(\textbf{x})),

where ηsi (x) is the error for class ωs made by classifier Di . 


##### Pairwise diversity measures

###### The Q statistics

Considering two classifiers Di and Dk, such that yj,i = 1, if Di recognizes correctly zj, and 0, otherwise, i = 1,..., L. 

You could think it resembles a confusion matrix in such a way that you compare the correctness of the Di and Dk classifiers pair by pair, counting the numbers of total corrected and incorrected classifications made by those models. 

The Yule's Q-statistics for the two classifiers before mentioned, Di and Dk, is

![Q_{i,k}=\frac{N^{11}N^{00}-N^{01}N^{10}}{N^{11}N^{00}+N^{01}N^{10}}](https://latex.codecogs.com/gif.latex?Q_{i,k}=\frac{N^{11}N^{00}-N^{01}N^{10}}{N^{11}N^{00}+N^{01}N^{10}}),

where N<sup>ab</sup> is the number of elements z<sub>j</sub> of Z for which y<sub>j,i</sub> = a and y<sub>j,k</sub> = b. For statistically independent classifiers, the expectation of Q<sub>i,k</sub> is 0. Q varies between −1 and 1. 

When considering a team *D* of *L* classifiers, the averegaed *Q* statistics over all pairs of classifiers is, 

![Q_{av}=\frac{2}{L(L-1)}\sum_{i=1}^{L-1}\sum_{k=i+1}^{L}Q_{i,k}](https://latex.codecogs.com/gif.latex?Q_{av}=\frac{2}{L(L-1)}\sum_{i=1}^{L-1}\sum_{k=i+1}^{L}Q_{i,k}).

###### The correlation coefficient ![\rho](https://latex.codecogs.com/gif.latex?\rho)

The correlation between two binary variables (i.e., classifier outputs) which represents correct or incorrect, **y<sub>i</sub>** and **y<sub>k</sub>**, is 

![\rho_{i,k}=\frac{N^{11}N^{00}-N^{01}N^{10}}{\sqrt{(N^{11}+N^{10})(N^{01}+N^{00})(N^{11}+N^{01})(N^{10}+N^{00})}}](https://latex.codecogs.com/gif.latex?\rho_{i,k}=\frac{N^{11}N^{00}-N^{01}N^{10}}{\sqrt{(N^{11}+N^{10})(N^{01}+N^{00})(N^{11}+N^{01})(N^{10}+N^{00})}})

Kuncheva affirms that for any two classifiers, *Q* and ![\rho](https://latex.codecogs.com/gif.latex?\rho) have the same sign, and it can be proved that ![|\rho|\le|Q|](https://latex.codecogs.com/gif.latex?|\rho|\le|Q|). Also, she emphasizes that *Q* statistics was chosen because it is simpler than the correlation, and it has been designed for contingency tables, which could improve the analysis.


**Reference**


KUNCHEVA, Ludmila I. Combining Pattern Classifiers. Methods and Algorithms: Wiley, 2014.

KUNCHEVA, Ludmila I.; WHITAKER, Christopher J. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine learning, v. 51, n. 2, p. 181-207, 2003.
