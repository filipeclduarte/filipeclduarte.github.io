# Ensemble Learning

According to Kuncheva (2003), "combining classifiers is now an established research area known under different names in the literature: committees of learners, mixtures of experts, classifier ensembles, multiple classifier systems, consensus theory, etc."

Combine models achieve better results than using one model for classification Kuncheva (2014). Also, an ensemble classifier is just one classifier model.

**Reasons to combine models**

 I. **Statistical**: It is better to average several models than choose only one. 
 
 II. **Computational**: Small sample size or too much data; Imperfect training algorithm. 
 
 III. **Representational**: Complex boundary can be approximated with the desired precision by simple boundaries. An ensemble of linear classifiers can approximate a highly nonlinear classification boundary. 

**Ensemble Learning consists of three phases**

1. Generation

 It is preferable to use simpler models like Perceptrons or Decision Trees for the pool to guarantee diversity. We could use a "monolithic" model or not, for example, using several different algorithms. 
  
**Bagging** is an ensemble that combines multiple models based on bootstrap samples. In this sense, it is possible to use a monolith or several different classifiers algorithms. 

**Random subspace** is an ensemble that combines multiple models based on random feature selection. For each classifier in an 'L' pool of classifiers, sample 'n' features to training. 

2. Selection

In classifier **Selection**, each classifier is supposed to know some part of the feature space. 

3. Fusion

In classifier **Fusion**, each classifier is supposed to know the whole feature space. 



**Random Linear Oracle**

The Random Linear Oracle (RLO) is a method that combines the classifier fusion and classifier selection approaches (Kuncheva, 2014). Linear Oracle is a mini-ensemble consisting of only two classifiers, which choose the best classifier for a given training instance. 

![Kuncheva (2014)](/images/RLO.png)


### Measures of Diversity

Diversity is an essential characteristic in ensemble learning (KUNCHEVA, 2003, Cunningham & Carney, 2000; Krogh & Vedelsby, 1995; Rosen, 1996; Lam, 2000; Littlewood & Miller, 1989). However, there is no formal definition of what it represents. In this sense, Kuncheva (2003) tries to resolve these issues and answer some questions about this topic, such as i) how we define diversity? ii) How are the various measures of diversity-related to each other? iii) How are the measures related to the accuracy of the team? iv) Is there a measure that is best to develop committees that minimize error? v) How can we use the measures in designing the classifier ensemble?


#### Diversity in classifier ensemble 

Assume that classifier outputs are estimates of the posterior probabilities, ![P(\omega|\textbf{x})](https://latex.codecogs.com/gif.latex?P(\omega|\textbf{x})), s = 1, ..., c, so that the estimate satisfies:

![\hat{P}_i(\omega_s|\textbf{x})=P(\omega_s|\textbf{x})+\eta_s^i(\textbf{x})](https://latex.codecogs.com/gif.latex?\hat{P}_i(\omega_s|\textbf{x})=P(\omega_s|\textbf{x})+\eta_s^i(\textbf{x})),

where ηsi (x) is the error for class ωs made by classifier Di . 




**Reference**


KUNCHEVA, Ludmila I. Combining Pattern Classifiers. Methods and Algorithms: Wiley, 2014.

KUNCHEVA, Ludmila I.; WHITAKER, Christopher J. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine learning, v. 51, n. 2, p. 181-207, 2003.
