# Ensemble Learning

Combine models achieve better results than using one model for classification Kuncheva (2014). Also, an ensemble classifier is just one classifier model.

**Reasons to combine models**

 I. **Statistical**: It is better to average several models than choose only one. 
 
 II. **Computational**: Small sample size or too much data; Imperfect training algorithm. 
 
 III. **Representational**: Complex boundary can be approximated with the desired precision by simple boundaries. An ensemble of linear classifiers can approximate a highly nonlinear classification boundary. 

**Ensemble Learning consists of three phases**

1. Generation

 It is preferable to use simpler models like Perceptrons or Decision Trees for the pool to guarantee diversity. We could use a "monolithic" model or not, for example, using several different algorithms. 
  
**Bagging** is an ensemble that combines multiple models based on bootstrap samples. In this sense, it is possible to use a monolith or several different classifiers algorithms. 

**Random subspace** is an ensemble that combines multiple models based on random feature selection. For each classifier in an 'L' pool of classifiers, sample 'n' features to training. 

2. Selection

In classifier **Selection**, each classifier is supposed to know some part of the feature space. 

3. Fusion

In classifier **Fusion**, each classifier is supposed to know the whole feature space. 



**Random Linear Oracle**

The Random Linear Oracle (RLO) is a method that combines the classifier fusion and classifier selection approaches (Kuncheva, 2014). Linear Oracle is a mini-ensemble consisting of only two classifiers, which choose the best classifier for a given training instance. 

![Kuncheva (2014)](/images/RLO.png)


**Reference**

Kuncheva, Ludmila I. Combining Pattern Classifiers. Methods and Algorithms: Wiley, 2014.
